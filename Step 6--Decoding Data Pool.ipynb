{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c588b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reads: 13487\n",
      "This is Chapter 0\n",
      "Digital production, transmission and storage have revolutionized how we access and use information but have also made archiving an increasingly complex task that requires active, continuing maintenance of digital media. This challenge has focused some interest on DNA as an attractive target for information storage because of its capacity for high-density information encoding, longevity under easily achieved conditions and proven track record as an information bearer. Previous DNA-based information storage approaches have encoded only trivial amounts of information or were not amenable to scaling-up, and used no robust error-correction and lacked examination of their cost-efficiency for large-scale information archival. Here we describe a scalable method that can reliably store more information than has been handled before. We encoded computer files totalling 739 kilobytes of hard-disk storage and with an estimated Shannon information of 5.2 x 10^6 bits into a DNA code, synthesized this DNA, sequenced it and reconstructed the original files with 100% accuracy. Theoretical analysis indicates that our DNA-based storage scheme could be scaled far beyond current global information volumes and offers a realistic technology for large-scale, long-term and infrequently accessed digital archiving. In fact, current trends in technological advances are reducing DNA synthesis costs at a pace that should make our scheme cost-effective for sub-50-year archiving within a decade.\n",
      "This is Chapter 2\n",
      "DNA storage offers substantial information density and exceptional half-life. We devised a 'DNA-of-things' (DoT) storage architecture to produce materials with immutable memory. In a DoT framework, DNA molecules record the data, and these molecules are then encapsulated in nanometer silica beads, which are fused into various materials that are used to print or cast objects in any shape. First, we applied DoT to three-dimensionally print a Stanford Bunny(9) that contained a 45 kB digital DNA blueprint for its synthesis. We synthesized five generations of the bunny, each from the memory of the previous generation without additional DNA synthesis or degradation of information. To test the scalability of DoT, we stored a 1.4 MB video in DNA in plexiglass spectacle lenses and retrieved it by excising a tiny piece of the plexiglass and sequencing the embedded DNA. DoT could be applied to store electronic health records in medical implants, to hide data in everyday objects (steganography) and to manufacture objects containing their own blueprint. It may also facilitate the development of self-replicating machines.A DNA-based method for embedding data in materials enables the conversion of everyday objects into data storage devices.\n",
      "This is Chapter 6\n",
      "While identifying acute HIV infection is critical to providing prompt treatment to HIV-positive individuals and preventing transmission, existing laboratory-based testing methods are too complex to perform at the point of care. Specifically, molecular techniques can detect HIV RNA within 8-10 days of transmission but require laboratory infrastructure for cold-chain reagent storage and extensive sample preparation performed by trained personnel. Here, we demonstrate our point-of-care microfluidic rapid and autonomous analysis device (microRAAD) that automatically detects HIV RNA from whole blood. Inside microRAAD, we incorporate vitrified amplification reagents, thermally-actuated valves for fluidic control, and a temperature control circuit for low-power heating. Reverse transcription loop-mediated isothermal amplification (RT-LAMP) products are visualized using a lateral flow immunoassay (LFIA), resulting in an assay limit of detection of 100 HIV-1 RNA copies when performed as a standard tube reaction. Even after three weeks of room-temperature reagent storage, microRAAD automatically isolates the virus from whole blood, amplifies HIV-1 RNA, and transports amplification products to the internal LFIA, detecting as few as 3 x 10^5 HIV-1 viral particles, or 2.3 x 10^7 virus copies per mL of whole blood, within 90 minutes. This integrated microRAAD is a low-cost and portable platform to enable automated detection of HIV and other pathogens at the point of care. \n",
      "This is Chapter 7\n",
      "Synthetic DNA is durable and can encode digital data with high density, making it an attractive medium for data storage. However, recovering stored data on a large-scale currently requires all the DNA in a pool to be sequenced, even if only a subset of the information needs to be extracted. Here, we encode and store 35 distinct files (over 200 MB of data), in more than 13 million DNA oligonucleotides, and show that we can recover each file individually and with no errors, using a random access approach. We design and validate a large library of primers that enable individual recovery of all files stored within the DNA. We also develop an algorithm that greatly reduces the sequencing read coverage required for error-free decoding by maximizing information from all sequence reads. These advances demonstrate a viable, large-scale system for DNA data storage and retrieval. \n",
      "This is Chapter 16\n",
      "As global demand for digital storage capacity grows, storage technologies based on synthetic DNA have emerged as a dense and durable alternative to traditional media. Existing approaches leverage robust error correcting codes and precise molecular mechanisms to reliably retrieve specific files from large databases. Typically, files are retrieved using a pre-specified key, analogous to a filename. However, these approaches lack the ability to perform more complex computations over the stored data, such as similarity search: e.g., finding images that look similar to an image of interest without prior knowledge of their file names. Here we demonstrate a technique for executing similarity search over a DNA-based database of 1.6 million images. Queries are implemented as hybridization probes, and a key step in our approach was to learn an image-to-sequence encoding ensuring that queries preferentially bind to targets representing visually similar images. Experimental results show that our molecular implementation performs comparably to state-of-the-art in silico algorithms for similarity search. \n",
      "Chapter 0 is found\n",
      "Last segment is 52\n",
      "Chapter 2 is found\n",
      "Last segment is 42\n",
      "Chapter 6 is found\n",
      "Last segment is 52\n",
      "Chapter 7 is found\n",
      "Last segment is 31\n",
      "Chapter 16 is found\n",
      "Last segment is 38\n",
      "Total number of valid reads: 12309\n",
      "Exact matched sequences before RS correction: 8422\n",
      "Matched sequences after RS correction: 9138\n"
     ]
    }
   ],
   "source": [
    "# This program aims to process the data sequences and translate them into ascii texts\n",
    "# Data sequence processing workflow:\n",
    "# 1. Identify and exclude PCR primers. The PCR primers also corresponds to the Chapter number.\n",
    "# 2. RS correcting the data sequence and extract data sequence.\n",
    "# 3. Decode segment number (the first four bases after forward primer).\n",
    "# 4. Convert data sequence to hex data.\n",
    "# 5. Convert hex data to texts\n",
    "\n",
    "\n",
    "from reedsolo import RSCodec, ReedSolomonError\n",
    "\n",
    "rsc = RSCodec(2)\n",
    "\n",
    "%store -r primerLibrary\n",
    "%store -r referenceStrands\n",
    "%store -r array_data_payload\n",
    "\n",
    "def converter(seq):\n",
    "    converter = {'A': '00', 'C': '01', 'G': '10', 'T': '11'} \n",
    "    bases = list(seq) \n",
    "    bases = [converter[base] for base in bases] \n",
    "    return ''.join(bases)\n",
    "\n",
    "def deconverter(seq):\n",
    "    deconverter = {'00': 'A', '01': 'C', '10': 'G', '11': 'T'} \n",
    "    doubleBits = [seq[i:i+2] for i in range(0, len(seq), 2)]\n",
    "    doubleBits = [deconverter[doubleBit] for doubleBit in doubleBits] \n",
    "    return ''.join(doubleBits)\n",
    "\n",
    "def oligoToBase3(seq):\n",
    "    oligoToBase3 = {'G': '0', 'T': '1', 'A': '2', 'C': '3', 'N': '3'}\n",
    "    oligoToBase3Converted = []\n",
    "    for i in range (0, len(seq)):\n",
    "        if (oligoToBase3[seq[i]] == '3'):\n",
    "            return -1\n",
    "        else:\n",
    "            oligoToBase3Converted.append(oligoToBase3[seq[i]])\n",
    "            if seq[i] == 'C':\n",
    "                oligoToBase3 = {'G': '0', 'T': '1', 'A': '2', 'C': '3', 'N': '3'}\n",
    "            elif seq[i] == 'G':\n",
    "                oligoToBase3 = {'T': '0', 'A': '1', 'C': '2', 'G': '3', 'N': '3'}\n",
    "            elif seq[i] == 'T':\n",
    "                oligoToBase3 = {'A': '0', 'C': '1', 'G': '2', 'T': '3', 'N': '3'}\n",
    "            elif seq[i] == 'A':\n",
    "                oligoToBase3 = {'C': '0', 'G': '1', 'T': '2', 'A': '3', 'N': '3'}\n",
    "\n",
    "\n",
    "    return ''.join(oligoToBase3Converted)\n",
    "\n",
    "def ternaryToDecimal(n):\n",
    "    decimal = 0\n",
    "    n = ''.join(reversed(n))\n",
    "    for i in range (0, len(n)):\n",
    "        decimal += (int(n[i]))*(pow(3, i))\n",
    "    return decimal\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    string = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            string = i\n",
    "            \n",
    "    return string\n",
    "\n",
    "def binaryToHex(binary_string):\n",
    "    decimal_representation = int(binary_string, 2)\n",
    "    hexadecimal_string = hex(decimal_representation)\n",
    "    return hexadecimal_string\n",
    "\n",
    "def hexToText(hex_string):\n",
    "    hex_string = hex_string[2:]\n",
    "    bytes_object = bytes.fromhex(hex_string)\n",
    "    ascii_string = bytes_object.decode(\"utf-8\")\n",
    "    return ascii_string\n",
    "\n",
    "\n",
    "\n",
    "%store -r array_data\n",
    "\n",
    "# Read input FASTQ file\n",
    "file_R1 = open('Raw reads/data_1_S1_L001_R1_001.fastq', 'r')\n",
    "file_R2 = open('Raw reads/data_1_S1_L001_R2_001.fastq', 'r')\n",
    "\n",
    "Lines_R1 = file_R1.readlines()\n",
    "Lines_R2 = file_R2.readlines()\n",
    "\n",
    "new_Lines_R1 = []\n",
    "new_Lines_R2 = []\n",
    "\n",
    "complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "\n",
    "for i in range (0, int(len(Lines_R1))):\n",
    "    if i%4 == 1:\n",
    "        new_Lines_R1.append(Lines_R1[i][0:151].strip('\\n'))\n",
    "\n",
    "for i in range (0, int(len(Lines_R2))):\n",
    "    if i%4 == 1:\n",
    "        reverse_complement_R2 = \"\".join(complement.get(base, base) for base in reversed(Lines_R2[i][0:151].strip('\\n')))\n",
    "        new_Lines_R2.append(reverse_complement_R2)\n",
    "\n",
    "print('Total number of reads: ' + str(len(new_Lines_R1)))\n",
    "\n",
    "new_Lines_Combo = []\n",
    "\n",
    "# Define a list to store Read Number of those reads which passed the following conditions:\n",
    "read_index_1 = []\n",
    "\n",
    "for i in range (0, len(new_Lines_R1)):\n",
    "    \n",
    "    # Pick out those with 149-nt read overlap \n",
    "    if (new_Lines_R1[i][0:149] == new_Lines_R2[i][0:149]): # Ensure paired-end read generates consistent results.\n",
    "        new_Lines_Combo.append(new_Lines_R1[i][0:149])\n",
    "        read_index_1.append(i)\n",
    "        \n",
    "    # If the length of payload is exactly 142 nt, some reads may be excluded from the above condition because they have mismatches outside payload region.\n",
    "    # Then we should pick out those with 142-nt read overlap. \n",
    "    elif (new_Lines_R1[i][0:142] == new_Lines_R2[i][0:142]): # Ensure paired-end read generates consistent results.\n",
    "        new_Lines_Combo.append(new_Lines_R1[i][0:142])\n",
    "        read_index_1.append(i)\n",
    "\n",
    "    # If payload is longer than 151 nt, then we have to find overlap between the two reads\n",
    "    else:                                  \n",
    "        for j in range (0, len(new_Lines_R1[i])):\n",
    "            if (new_Lines_R1[i][j:] == new_Lines_R2[i][0:len(new_Lines_R1[i])-j]): # Detecting maximal overlapped region\n",
    "                new_Lines_Combo_Unit = new_Lines_R1[i] + new_Lines_R2[i][-(j+len(new_Lines_R2[i])-len(new_Lines_R1[i])):]\n",
    "\n",
    "                # Exclude those longer than 170 nt.\n",
    "                if len(new_Lines_Combo_Unit) > 170:\n",
    "                    continue \n",
    "\n",
    "                new_Lines_Combo.append(new_Lines_Combo_Unit)\n",
    "                read_index_1.append(i)\n",
    "                break\n",
    "        \n",
    "        \n",
    "Lines = new_Lines_Combo\n",
    "\n",
    "# Get the number of valid reads\n",
    "res_read_index_1 = [*set(read_index_1)]\n",
    "\n",
    "\n",
    "primerLength = 21\n",
    "dataStrandsOccup = [[None]*100 for _ in range(40)]\n",
    "dataStrandstoText = [[None]*100 for _ in range(40)]\n",
    "dataStrands = [[None]*100 for _ in range(40)]\n",
    "candi_dataStrands = [ [ [] for i in range(100) ] for i in range(40) ]\n",
    "\n",
    "RS_before = 0\n",
    "RS_after = 0\n",
    "read_index_2 = []\n",
    "read_index_3 = []\n",
    "\n",
    "read_number = -1\n",
    "\n",
    "for data in Lines:\n",
    "    read_number += 1\n",
    "    data = data.strip('\\n')   # In '.txt' files, there may be '\\n' symbols meaning the start of a new line. Those symbols needs to be eliminated. \n",
    "    # Extract forward and reverse PCR primer sequence and identify the Chapter Number\n",
    "    chapterStartSequence = data[0:primerLength]\n",
    "    chapterEndSequence = data[(len(data) - primerLength):]\n",
    "    try:\n",
    "        chapterStart = primerLibrary.index(chapterStartSequence)\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        chapterEnd = primerLibrary.index(chapterEndSequence)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    chapterEnd = len(primerLibrary) - chapterEnd - 1\n",
    "    if (chapterStart == chapterEnd):\n",
    "        Chapter = chapterStart\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    data = data[primerLength:(len(data)-primerLength)]\n",
    "    # Count matched sequence before RS correction\n",
    "    for m in range (0, 40):\n",
    "        for n in range (0, 100):\n",
    "            if data == array_data[m][n]:\n",
    "                read_index_2.append(read_index_1[read_number])   # Store Read Number with successful decoding attempt before RS correction\n",
    "    \n",
    "    RS = data[-12:]  # Extract RS Sequence (base-3)\n",
    "    \n",
    "    data_to_be_modulated = data[4:(len(data)-12)]\n",
    "    data_modulated = ''\n",
    "    for i in range (0, int(len(data_to_be_modulated)/7)):\n",
    "        data_modulated_segment = data_to_be_modulated[i*7:(i+1)*7]\n",
    "        data_modulated_segment = data_modulated_segment + data_to_be_modulated[i*7+6]  # Add one more Pointer Base to make the sequence checkable by RS\n",
    "        data_modulated = data_modulated + data_modulated_segment\n",
    "        data_modulated_segment = ''\n",
    "    \n",
    "    data = data[0:4] + data_modulated\n",
    "    \n",
    "    # Convert RS Sequence to ternary number, then to binary number\n",
    "    RS_segment_binary_total = []\n",
    "    for i in range (0, int(len(RS)/6)):\n",
    "        RS_segment = RS[i*6:(i+1)*6]\n",
    "        RS_segment_base3 = oligoToBase3(RS_segment)\n",
    "        if (RS_segment_base3 == -1):\n",
    "            break\n",
    "        RS_segment_decimal = ternaryToDecimal(RS_segment_base3)\n",
    "        RS_segment_binary = bin(RS_segment_decimal)\n",
    "        RS_segment_binary = RS_segment_binary[2:]\n",
    "        RS_segment_binary = '0'*(8-len(RS_segment_binary)) + RS_segment_binary\n",
    "        RS_segment_binary = str(RS_segment_binary)\n",
    "        RS_segment_binary_total.append(RS_segment_binary)\n",
    "    RS_segment_binary_total = ''.join(RS_segment_binary_total)\n",
    "\n",
    "    if (len(RS_segment_binary_total) != 16):\n",
    "        continue\n",
    "    \n",
    "    binaryConverted = converter(data)    # Convert Data Sequence to binary number \n",
    "    binaryConverted = binaryConverted + RS_segment_binary_total  # Combine the data and RS code in binary form\n",
    "\n",
    "    binaryList = [int(binaryConverted[i:i + 8], 2) for i in range(0, len(binaryConverted), 8)]\n",
    "    bytesList = bytes(binaryList)\n",
    "\n",
    "    try:\n",
    "        RSDecoded = rsc.decode(bytesList)[0]   # RS correction\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    bytes_as_bits = ''.join(format(byte, '08b') for byte in RSDecoded)\n",
    "    baseDeconverted = deconverter(bytes_as_bits)   # Convert corrected bytes back to DNA sequences \n",
    "    # Extract and identify Segment Number\n",
    "    segmentSequence = baseDeconverted[0:4]   \n",
    "    segmentBase3 = oligoToBase3(segmentSequence)\n",
    "    if (segmentBase3 == -1):\n",
    "        continue\n",
    "    segmentBase10 = ternaryToDecimal(segmentBase3)\n",
    "    Segment = segmentBase10\n",
    "\n",
    "    \n",
    "    # In each segment, get Index Bases and Pointer Base in 8-nt unit\n",
    "    baseDeconverted = baseDeconverted[4:]\n",
    "    corrected_data = ''\n",
    "    characterLength = int(len(baseDeconverted)/8)\n",
    "    \n",
    "    for i in range (0, characterLength):\n",
    "        characterWhole = baseDeconverted[i*8:(i+1)*8]\n",
    "        characterIndexSeq = characterWhole[0:6]\n",
    "        corrected_data += characterIndexSeq\n",
    "        characterPointerSeq = characterWhole[6]\n",
    "        corrected_data += characterPointerSeq\n",
    "    \n",
    "    # Count matched sequence after RS correction\n",
    "    for m in range (0, 40):\n",
    "        for n in range (0, 100):\n",
    "            if corrected_data == array_data_payload[m][n]:\n",
    "                dataStrandsOccup[Chapter][Segment] = 1\n",
    "                read_index_3.append(read_index_1[read_number])   # Store Read Number with successful decoding attempt after RS correction\n",
    "    \n",
    "    candi_dataStrands[Chapter][Segment].append(corrected_data) # Store the RS-corrected sequence in candidate sequence list\n",
    "\n",
    "\n",
    "for m in range (0, 40):\n",
    "    for n in range (0, 100):\n",
    "        if (dataStrands[m][n] == None):\n",
    "            try:\n",
    "                dataStrands[m][n] = most_frequent(candi_dataStrands[m][n])  # Only keep the sequence appearing most frequently\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "for m in range (0, 40):\n",
    "    for n in range (0, 100):      \n",
    "        if (dataStrands[m][n] != None):\n",
    "            baseDeconverted = dataStrands[m][n]  \n",
    "            getSegmentData = []\n",
    "            characterLength = int(len(baseDeconverted)/7)\n",
    "            for i in range (0, characterLength):\n",
    "                characterWhole = baseDeconverted[i*7:(i+1)*7]\n",
    "                characterIndexSeq = characterWhole[0:6]\n",
    "                characterPointerSeq = characterWhole[6]\n",
    "                ternaryCharacterIndex = oligoToBase3(characterIndexSeq)\n",
    "                if (ternaryCharacterIndex == -1):\n",
    "                    continue\n",
    "                characterIndex = ternaryToDecimal(ternaryCharacterIndex)   # Convert ternary Index Number to decimal number\n",
    "\n",
    "                if (characterIndex > 720):\n",
    "                    continue\n",
    "\n",
    "                # Locate Combination by looking up the Index Number and Pointer from the decoded Reference Strands\n",
    "                getReference = []\n",
    "                for i in range (0, len(referenceStrands)):\n",
    "                    getReference.append(referenceStrands[i][characterIndex])\n",
    "                getReference = ''.join(getReference)\n",
    "                getCharacterData = [None]*16\n",
    "                for i in range (0, len(getReference)):\n",
    "                    if getReference[i] == characterPointerSeq:\n",
    "                        getCharacterData[i] = '1'\n",
    "                    else:\n",
    "                        getCharacterData[i] = '0'\n",
    "                getCharacterData = ''.join(getCharacterData)\n",
    "                getSegmentData.append(getCharacterData)\n",
    "            getSegmentData = ''.join(getSegmentData)\n",
    "\n",
    "\n",
    "            # Eliminate redundant '00100000's which adds up oligo length to ensure the quality of batch production in oligo synthesis\n",
    "            while (len(getSegmentData) > 0):\n",
    "                if (getSegmentData[-16:-8] == '00100000'):\n",
    "                    getSegmentData = getSegmentData[0: len(getSegmentData)-8]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Convert binary data to hex data\n",
    "            getSegmentData_hex = binaryToHex(getSegmentData)\n",
    "\n",
    "            # Convert hex data to ASCII texts\n",
    "            try:\n",
    "                getSegmentData_text = hexToText(getSegmentData_hex)\n",
    "                dataStrandstoText[m][n] = getSegmentData_text \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "for i in range (0, len(dataStrandstoText)):\n",
    "    \n",
    "    text = ''\n",
    "    for j in range (0, len(dataStrandstoText[i])):\n",
    "        try:\n",
    "            text += dataStrandstoText[i][j]\n",
    "        except:\n",
    "            pass\n",
    "    if (text != ''):\n",
    "        print('This is Chapter ' + str(i))\n",
    "        print(text)\n",
    "\n",
    "\n",
    "for i in range (0, len(dataStrandsOccup)):\n",
    "    for j in range (0, len(dataStrandsOccup[i])):\n",
    "        if dataStrandsOccup[i][j] == 1:\n",
    "            if dataStrandsOccup[i][j+1] == None: \n",
    "                print('Chapter ' + str(i) + \" is found\")\n",
    "                print(\"Last segment is \" + str(j))\n",
    "                break\n",
    "    \n",
    "    \n",
    "# Get the number of successfully decoded reads before and after RS correction    \n",
    "res_read_index_2 = [*set(read_index_2)]\n",
    "res_read_index_3 = [*set(read_index_3)]\n",
    "print('Total number of valid reads: ' + str(len(res_read_index_1)))\n",
    "print('Exact matched sequences before RS correction: ' + str(len(res_read_index_2)))\n",
    "print('Matched sequences after RS correction: ' + str(len(res_read_index_3)))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6722e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
