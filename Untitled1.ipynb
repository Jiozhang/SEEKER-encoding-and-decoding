{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a90eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reads: 7069\n",
      "This is Chapter 0\n",
      "Digital production, transmission and storage have revolutionized how we access and use information but have also made archiving an increasingly complex task that requires active, continuing maintenance of digital media. This challenge has focused some interest on DNA as an attractive target for information storage because of its capacity for high-density information encoding, longevity under easily achieved conditions and proven track record as an information bearer. Previous DNA-based information storage approaches have encoded only trivial amounts of information or were not amenable to scaling-up, and used no robust error-correction and lacked examination of their cost-efficiency for large-scale information archival. Here we describe a scalable method that can reliably store more information than has been handled before. We encoded computer files totalling 739 kilobytes of hard-disk storage and with an estimated Shannon information of 5.2 x 10^6 bits into a DNA code, synthesized this DNA, sequenced it and reconstructed the original files with 100% accuracy. Theoretical analysis indicates that our DNA-based storage scheme could be scaled far beyond current global information volumes and offers a realistic technology for large-scale, long-term and infrequently accessed digital archiving. In fact, current trends in technological advances are reducing DNA synthesis costs at a pace that should make our scheme cost-effective for sub-50-year archiving within a decade.\n",
      "This is Chapter 2\n",
      "these molecules are then \n",
      "This is Chapter 3\n",
      "The multispike tempotron (MST) is a powerful, single spiking neuron model that can solve complex supervised classification tasks. It is also internally complex, computationally expensive to evaluate, and unsuitable for neuromorphic hardware. Here we aim to understand whether it is possible to simplify the MST model while retaining its ability to learn and process information. To this end, we introduce a family of generalized neuron models (GNMs) that are a special case of the spike response model and much simpler and cheaper to simulate than the MST. We find that over a wide range of parameters, the GNM can learn at least as well as the MST does. We identify the temporal autocorrelation of the membrane potential as the most important ingredient of the GNM that enables it to classify multiple spatiotemporal patterns. We also interpret the GNM as a chemical system, thus conceptually bridging computation by neural networks with molecular information processing. We conclude the letter by proposing alternative training approaches for the GNM, including error trace learning and error backpropagation. \n",
      "This is Chapter 16\n",
      "As global demand for digital storage capacity grows, storage technologies based on synthetic DNA have emerged as a dense and durable alternative to traditional media. Existing approaches leverage robust error correcting codes and precise molecular mechanisms to reliably retrieve specific files from large databases. Typically, files are retrieved using a pre-specified key, analogous to a filename. However, these approaches lack the ability to perform more complex computations over the stored data, such as similarity search: e.g., finding images that look similar to an image of interest without prior knowledge of their file names. Here we demonstrate a technique for executing similarity search over a DNA-based database of 1.6 million images. Queries are implemented as hybridization probes, and a key step in our approach was to learn an image-to-sequence encoding ensuring that queries preferentially bind to targets representing visually similar images. Experimental results show that our molecular implementation performs comparably to state-of-the-art in silico algorithms for similarity search. \n",
      "This is Chapter 21\n",
      "Delay-sensitive applications have been driving the move away from cloud computing, which cannot meet their low-latency requirements. Edge computing and programmable switches have been among the first steps toward pushing computation closer to end-users in order to reduce cost, latency, and overall resource utilization. This article presents the \"compute-less\" paradigm, which builds on top of the well known edge computing paradigm through a set of communication and computation optimization mechanisms (e.g.,, in-network computing, task clustering and aggregation, computation reuse). The main objective of the compute-less paradigm is to reduce the migration of computation and the usage of network and computing resources, while maintaining high Quality of Experience for end-users. We discuss the new perspectives, challenges, limitations, and opportunities of this compute-less paradigm.\n",
      "This is Chapter 29\n",
      "Syndromic genetic conditions, in aggregate, affect 8% of the population. Many syndromes have recognizable facial features that are highly informative to clinical geneticists. Recent studies show that facial analysis technologies measured up to the capabilities of expert clinicians in syndrome identification. However, these technologies identified only a few disease phenotypes, limiting their role in clinical settings, where hundreds of diagnoses must be considered. Here we present a facial image analysis framework, DeepGestalt, using computer vision and deep-learning algorithms, that quantifies similarities to hundreds of syndromes. DeepGestalt outperformed clinicians in three initial experiments, two with the goal of distinguishing subjects with a target syndrome from other syndromes, and one of separating different genetic sub-types in Noonan syndrome. On the final experiment reflecting a real clinical setting problem, DeepGestalt achieved 91% top-10 accuracy in identifying the correct syndrome on 502 different images. The model was trained on a dataset of over 17,000 images representing more than 200 syndromes, curated through a community-driven phenotyping platform. DeepGestalt potentially adds considerable value to phenotypic evaluations in clinical genetics, genetic testing, research and precision medicine. \n",
      "This is Chapter 33\n",
      "ViruSurf, available at http://gmql.eu/virusurf/, is a large public database of viral sequences and integrated and curated metadata from heterogeneous sources (RefSeq, GenBank, COG-UK and NMDC); it also exposes computed nucleotide and amino acid variants, called from original sequences. A GISAID-specific ViruSurf database, available at http://gmql. eu/virusurf gisaid/, offers a subset of these functionalities. Given the current pandemic outbreak, SARS-CoV-2 data are collected from the four sources; but ViruSurf contains other virus species harmful to humans, including SARS-CoV, MERS-CoV, Ebola and Dengue. The database is centered on sequences, described from their biological, technological and organizational dimensions. In addition, the analytical dimension characterizes the sequence in terms of its annotations and variants. The web interface enables expressing complex search queries in a simple way; arbitrary search queries can freely combine conditions on attributes from the four dimensions, extracting the resulting sequences. Several example queries on the database confirm and possibly improve results from recent research papers; results can be recomputed over time and upon selected populations. Effective search over large and curated sequence data may enable faster responses to future threats that could arise from new viruses. \n",
      "This is Chapter 38\n",
      "Quantum computing by nuclear magnetic resonance using pseudopure spin states is bound by the maximal speed of quantum computing algorithms operating on pure states. In contrast to these quantum computing algorithms, a novel algorithm for searching an unsorted database is presented here that operates on truly mixed states in spin Liouville space. It provides an exponential speedup over Grover's quantum search algorithm with the sensitivity scaling exponentially with the number of spins, as for pseudopure state implementations. The minimal decoherence time required is exponentially shorter than that for Grover's algorithm. \n",
      "Chapter 0 is found\n",
      "Last segment is 52\n",
      "Chapter 2 is found\n",
      "Last segment is 8\n",
      "Chapter 3 is found\n",
      "Last segment is 39\n",
      "Chapter 16 is found\n",
      "Last segment is 38\n",
      "Chapter 21 is found\n",
      "Last segment is 30\n",
      "Chapter 29 is found\n",
      "Last segment is 46\n",
      "Chapter 33 is found\n",
      "Last segment is 48\n",
      "Chapter 38 is found\n",
      "Last segment is 21\n",
      "Total number of valid reads: 6742\n",
      "Exact matched sequences before RS correction: 4871\n",
      "Matched sequences after RS correction: 5251\n",
      "0\n",
      "657\n",
      "2\n",
      "1\n",
      "3\n",
      "586\n",
      "16\n",
      "641\n",
      "21\n",
      "876\n",
      "29\n",
      "791\n",
      "33\n",
      "809\n",
      "38\n",
      "910\n"
     ]
    }
   ],
   "source": [
    "# This program aims to process the data sequences and translate them into ascii texts\n",
    "# Data sequence processing workflow:\n",
    "# 1. Identify and exclude PCR primers. The PCR primers also corresponds to the Chapter number.\n",
    "# 2. RS correcting the data sequence and extract data sequence.\n",
    "# 3. Decode segment number (the first four bases after forward primer).\n",
    "# 4. Convert data sequence to hex data.\n",
    "# 5. Convert hex data to texts\n",
    "\n",
    "\n",
    "from reedsolo import RSCodec, ReedSolomonError\n",
    "\n",
    "rsc = RSCodec(2)\n",
    "\n",
    "%store -r primerLibrary\n",
    "%store -r referenceStrands\n",
    "%store -r array_data_payload\n",
    "\n",
    "def converter(seq):\n",
    "    converter = {'A': '00', 'C': '01', 'G': '10', 'T': '11'} \n",
    "    bases = list(seq) \n",
    "    bases = [converter[base] for base in bases] \n",
    "    return ''.join(bases)\n",
    "\n",
    "def deconverter(seq):\n",
    "    deconverter = {'00': 'A', '01': 'C', '10': 'G', '11': 'T'} \n",
    "    doubleBits = [seq[i:i+2] for i in range(0, len(seq), 2)]\n",
    "    doubleBits = [deconverter[doubleBit] for doubleBit in doubleBits] \n",
    "    return ''.join(doubleBits)\n",
    "\n",
    "def oligoToBase3(seq):\n",
    "    oligoToBase3 = {'G': '0', 'T': '1', 'A': '2', 'C': '3', 'N': '3'}\n",
    "    oligoToBase3Converted = []\n",
    "    for i in range (0, len(seq)):\n",
    "        if (oligoToBase3[seq[i]] == '3'):\n",
    "            return -1\n",
    "        else:\n",
    "            oligoToBase3Converted.append(oligoToBase3[seq[i]])\n",
    "            if seq[i] == 'C':\n",
    "                oligoToBase3 = {'G': '0', 'T': '1', 'A': '2', 'C': '3', 'N': '3'}\n",
    "            elif seq[i] == 'G':\n",
    "                oligoToBase3 = {'T': '0', 'A': '1', 'C': '2', 'G': '3', 'N': '3'}\n",
    "            elif seq[i] == 'T':\n",
    "                oligoToBase3 = {'A': '0', 'C': '1', 'G': '2', 'T': '3', 'N': '3'}\n",
    "            elif seq[i] == 'A':\n",
    "                oligoToBase3 = {'C': '0', 'G': '1', 'T': '2', 'A': '3', 'N': '3'}\n",
    "\n",
    "\n",
    "    return ''.join(oligoToBase3Converted)\n",
    "\n",
    "def ternaryToDecimal(n):\n",
    "    decimal = 0\n",
    "    n = ''.join(reversed(n))\n",
    "    for i in range (0, len(n)):\n",
    "        decimal += (int(n[i]))*(pow(3, i))\n",
    "    return decimal\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    string = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency > counter):\n",
    "            counter = curr_frequency\n",
    "            string = i\n",
    "            \n",
    "    return string\n",
    "\n",
    "def binaryToHex(binary_string):\n",
    "    decimal_representation = int(binary_string, 2)\n",
    "    hexadecimal_string = hex(decimal_representation)\n",
    "    return hexadecimal_string\n",
    "\n",
    "def hexToText(hex_string):\n",
    "    hex_string = hex_string[2:]\n",
    "    bytes_object = bytes.fromhex(hex_string)\n",
    "    ascii_string = bytes_object.decode(\"utf-8\")\n",
    "    return ascii_string\n",
    "\n",
    "\n",
    "\n",
    "%store -r array_data\n",
    "\n",
    "# Read input FASTQ file\n",
    "file_R1 = open('amplicon-4_S6_L001_R1_001.fastq', 'r')\n",
    "file_R2 = open('amplicon-4_S6_L001_R2_001.fastq', 'r')\n",
    "\n",
    "Lines_R1 = file_R1.readlines()\n",
    "Lines_R2 = file_R2.readlines()\n",
    "\n",
    "new_Lines_R1 = []\n",
    "new_Lines_R2 = []\n",
    "\n",
    "complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "\n",
    "for i in range (0, int(len(Lines_R1))):\n",
    "    if i%4 == 1:\n",
    "        new_Lines_R1.append(Lines_R1[i][0:151].strip('\\n'))\n",
    "\n",
    "for i in range (0, int(len(Lines_R2))):\n",
    "    if i%4 == 1:\n",
    "        reverse_complement_R2 = \"\".join(complement.get(base, base) for base in reversed(Lines_R2[i][0:151].strip('\\n')))\n",
    "        new_Lines_R2.append(reverse_complement_R2)\n",
    "\n",
    "print('Total number of reads: ' + str(len(new_Lines_R1)))\n",
    "\n",
    "new_Lines_Combo = []\n",
    "\n",
    "# Define a list to store Read Number of those reads which passed the following conditions:\n",
    "read_index_1 = []\n",
    "\n",
    "for i in range (0, len(new_Lines_R1)):\n",
    "    \n",
    "    # Pick out those with 149-nt read overlap \n",
    "    if (new_Lines_R1[i][0:149] == new_Lines_R2[i][0:149]): # Ensure paired-end read generates consistent results.\n",
    "        new_Lines_Combo.append(new_Lines_R1[i][0:149])\n",
    "        read_index_1.append(i)\n",
    "        \n",
    "    # If the length of payload is exactly 142 nt, some reads may be excluded from the above condition because they have mismatches outside payload region.\n",
    "    # Then we should pick out those with 142-nt read overlap. \n",
    "    elif (new_Lines_R1[i][0:142] == new_Lines_R2[i][0:142]): # Ensure paired-end read generates consistent results.\n",
    "        new_Lines_Combo.append(new_Lines_R1[i][0:142])\n",
    "        read_index_1.append(i)\n",
    "\n",
    "    # If payload is longer than 151 nt, then we have to find overlap between the two reads\n",
    "    else:                                  \n",
    "        for j in range (0, len(new_Lines_R1[i])):\n",
    "            if (new_Lines_R1[i][j:] == new_Lines_R2[i][0:len(new_Lines_R1[i])-j]): # Detecting maximal overlapped region\n",
    "                new_Lines_Combo_Unit = new_Lines_R1[i] + new_Lines_R2[i][-(j+len(new_Lines_R2[i])-len(new_Lines_R1[i])):]\n",
    "\n",
    "                # Exclude those longer than 170 nt.\n",
    "                if len(new_Lines_Combo_Unit) > 170:\n",
    "                    continue \n",
    "\n",
    "                new_Lines_Combo.append(new_Lines_Combo_Unit)\n",
    "                read_index_1.append(i)\n",
    "                break\n",
    "        \n",
    "        \n",
    "Lines = new_Lines_Combo\n",
    "\n",
    "# Get the number of valid reads\n",
    "res_read_index_1 = [*set(read_index_1)]\n",
    "\n",
    "\n",
    "primerLength = 21\n",
    "dataStrandsOccup = [[None]*100 for _ in range(40)]\n",
    "dataStrandstoText = [[None]*100 for _ in range(40)]\n",
    "dataStrands = [[None]*100 for _ in range(40)]\n",
    "candi_dataStrands = [ [ [] for i in range(100) ] for i in range(40) ]\n",
    "valid_reads = [ [ [0] for i in range(100) ] for i in range(40) ]\n",
    "\n",
    "RS_before = 0\n",
    "RS_after = 0\n",
    "read_index_2 = []\n",
    "read_index_3 = []\n",
    "\n",
    "read_number = -1\n",
    "\n",
    "for data in Lines:\n",
    "    read_number += 1\n",
    "    data = data.strip('\\n')   # In '.txt' files, there may be '\\n' symbols meaning the start of a new line. Those symbols needs to be eliminated. \n",
    "    # Extract forward and reverse PCR primer sequence and identify the Chapter Number\n",
    "    chapterStartSequence = data[0:primerLength]\n",
    "    chapterEndSequence = data[(len(data) - primerLength):]\n",
    "    try:\n",
    "        chapterStart = primerLibrary.index(chapterStartSequence)\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        chapterEnd = primerLibrary.index(chapterEndSequence)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    chapterEnd = len(primerLibrary) - chapterEnd - 1\n",
    "    if (chapterStart == chapterEnd):\n",
    "        Chapter = chapterStart\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    data = data[primerLength:(len(data)-primerLength)]\n",
    "    # Count matched sequence before RS correction\n",
    "    for m in range (0, 40):\n",
    "        for n in range (0, 100):\n",
    "            if data == array_data[m][n]:\n",
    "                read_index_2.append(read_index_1[read_number])   # Store Read Number with successful decoding attempt before RS correction\n",
    "    \n",
    "    RS = data[-12:]  # Extract RS Sequence (base-3)\n",
    "    \n",
    "    data_to_be_modulated = data[4:(len(data)-12)]\n",
    "    data_modulated = ''\n",
    "    for i in range (0, int(len(data_to_be_modulated)/7)):\n",
    "        data_modulated_segment = data_to_be_modulated[i*7:(i+1)*7]\n",
    "        data_modulated_segment = data_modulated_segment + data_to_be_modulated[i*7+6]  # Add one more Pointer Base to make the sequence checkable by RS\n",
    "        data_modulated = data_modulated + data_modulated_segment\n",
    "        data_modulated_segment = ''\n",
    "    \n",
    "    data = data[0:4] + data_modulated\n",
    "    \n",
    "    # Convert RS Sequence to ternary number, then to binary number\n",
    "    RS_segment_binary_total = []\n",
    "    for i in range (0, int(len(RS)/6)):\n",
    "        RS_segment = RS[i*6:(i+1)*6]\n",
    "        RS_segment_base3 = oligoToBase3(RS_segment)\n",
    "        if (RS_segment_base3 == -1):\n",
    "            break\n",
    "        RS_segment_decimal = ternaryToDecimal(RS_segment_base3)\n",
    "        RS_segment_binary = bin(RS_segment_decimal)\n",
    "        RS_segment_binary = RS_segment_binary[2:]\n",
    "        RS_segment_binary = '0'*(8-len(RS_segment_binary)) + RS_segment_binary\n",
    "        RS_segment_binary = str(RS_segment_binary)\n",
    "        RS_segment_binary_total.append(RS_segment_binary)\n",
    "    RS_segment_binary_total = ''.join(RS_segment_binary_total)\n",
    "\n",
    "    if (len(RS_segment_binary_total) != 16):\n",
    "        continue\n",
    "    \n",
    "    binaryConverted = converter(data)    # Convert Data Sequence to binary number \n",
    "    binaryConverted = binaryConverted + RS_segment_binary_total  # Combine the data and RS code in binary form\n",
    "\n",
    "    binaryList = [int(binaryConverted[i:i + 8], 2) for i in range(0, len(binaryConverted), 8)]\n",
    "    bytesList = bytes(binaryList)\n",
    "\n",
    "    try:\n",
    "        RSDecoded = rsc.decode(bytesList)[0]   # RS correction\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    bytes_as_bits = ''.join(format(byte, '08b') for byte in RSDecoded)\n",
    "    baseDeconverted = deconverter(bytes_as_bits)   # Convert corrected bytes back to DNA sequences \n",
    "    # Extract and identify Segment Number\n",
    "    segmentSequence = baseDeconverted[0:4]   \n",
    "    segmentBase3 = oligoToBase3(segmentSequence)\n",
    "    if (segmentBase3 == -1):\n",
    "        continue\n",
    "    segmentBase10 = ternaryToDecimal(segmentBase3)\n",
    "    Segment = segmentBase10\n",
    "\n",
    "    \n",
    "    # In each segment, get Index Bases and Pointer Base in 8-nt unit\n",
    "    baseDeconverted = baseDeconverted[4:]\n",
    "    corrected_data = ''\n",
    "    characterLength = int(len(baseDeconverted)/8)\n",
    "    \n",
    "    for i in range (0, characterLength):\n",
    "        characterWhole = baseDeconverted[i*8:(i+1)*8]\n",
    "        characterIndexSeq = characterWhole[0:6]\n",
    "        corrected_data += characterIndexSeq\n",
    "        characterPointerSeq = characterWhole[6]\n",
    "        corrected_data += characterPointerSeq\n",
    "    \n",
    "    # Count matched sequence after RS correction\n",
    "    for m in range (0, 40):\n",
    "        for n in range (0, 100):\n",
    "            if corrected_data == array_data_payload[m][n]:\n",
    "                dataStrandsOccup[Chapter][Segment] = 1\n",
    "                read_index_3.append(read_index_1[read_number])   # Store Read Number with successful decoding attempt after RS correction\n",
    "    \n",
    "    candi_dataStrands[Chapter][Segment].append(corrected_data) # Store the RS-corrected sequence in candidate sequence list\n",
    "    valid_reads[Chapter][Segment][0] += 1\n",
    "\n",
    "for m in range (0, 40):\n",
    "    for n in range (0, 100):\n",
    "        if (dataStrands[m][n] == None):\n",
    "            try:\n",
    "                dataStrands[m][n] = most_frequent(candi_dataStrands[m][n])  # Only keep the sequence appearing most frequently\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "for m in range (0, 40):\n",
    "    for n in range (0, 100):      \n",
    "        if (dataStrands[m][n] != None):\n",
    "            baseDeconverted = dataStrands[m][n]  \n",
    "            getSegmentData = []\n",
    "            characterLength = int(len(baseDeconverted)/7)\n",
    "            for i in range (0, characterLength):\n",
    "                characterWhole = baseDeconverted[i*7:(i+1)*7]\n",
    "                characterIndexSeq = characterWhole[0:6]\n",
    "                characterPointerSeq = characterWhole[6]\n",
    "                ternaryCharacterIndex = oligoToBase3(characterIndexSeq)\n",
    "                if (ternaryCharacterIndex == -1):\n",
    "                    continue\n",
    "                characterIndex = ternaryToDecimal(ternaryCharacterIndex)   # Convert ternary Index Number to decimal number\n",
    "\n",
    "                if (characterIndex > 720):\n",
    "                    continue\n",
    "\n",
    "                # Locate Combination by looking up the Index Number and Pointer from the decoded Reference Strands\n",
    "                getReference = []\n",
    "                for i in range (0, len(referenceStrands)):\n",
    "                    getReference.append(referenceStrands[i][characterIndex])\n",
    "                getReference = ''.join(getReference)\n",
    "                getCharacterData = [None]*16\n",
    "                for i in range (0, len(getReference)):\n",
    "                    if getReference[i] == characterPointerSeq:\n",
    "                        getCharacterData[i] = '1'\n",
    "                    else:\n",
    "                        getCharacterData[i] = '0'\n",
    "                getCharacterData = ''.join(getCharacterData)\n",
    "                getSegmentData.append(getCharacterData)\n",
    "            getSegmentData = ''.join(getSegmentData)\n",
    "\n",
    "\n",
    "            # Eliminate redundant '00100000's which adds up oligo length to ensure the quality of batch production in oligo synthesis\n",
    "            while (len(getSegmentData) > 0):\n",
    "                if (getSegmentData[-16:-8] == '00100000'):\n",
    "                    getSegmentData = getSegmentData[0: len(getSegmentData)-8]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Convert binary data to hex data\n",
    "            getSegmentData_hex = binaryToHex(getSegmentData)\n",
    "\n",
    "            # Convert hex data to ASCII texts\n",
    "            try:\n",
    "                getSegmentData_text = hexToText(getSegmentData_hex)\n",
    "                dataStrandstoText[m][n] = getSegmentData_text \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "for i in range (0, len(dataStrandstoText)):\n",
    "    \n",
    "    text = ''\n",
    "    for j in range (0, len(dataStrandstoText[i])):\n",
    "        try:\n",
    "            text += dataStrandstoText[i][j]\n",
    "        except:\n",
    "            pass\n",
    "    if (text != ''):\n",
    "        print('This is Chapter ' + str(i))\n",
    "        print(text)\n",
    "\n",
    "\n",
    "for i in range (0, len(dataStrandsOccup)):\n",
    "    for j in range (0, len(dataStrandsOccup[i])):\n",
    "        if dataStrandsOccup[i][j] == 1:\n",
    "            if dataStrandsOccup[i][j+1] == None: \n",
    "                print('Chapter ' + str(i) + \" is found\")\n",
    "                print(\"Last segment is \" + str(j))\n",
    "                break\n",
    "    \n",
    "    \n",
    "# Get the number of successfully decoded reads before and after RS correction    \n",
    "res_read_index_2 = [*set(read_index_2)]\n",
    "res_read_index_3 = [*set(read_index_3)]\n",
    "print('Total number of valid reads: ' + str(len(res_read_index_1)))\n",
    "print('Exact matched sequences before RS correction: ' + str(len(res_read_index_2)))\n",
    "print('Matched sequences after RS correction: ' + str(len(res_read_index_3)))\n",
    "\n",
    "\n",
    "for i in range (0, len(valid_reads)):\n",
    "    read_number = 0\n",
    "    for j in range (0, len(valid_reads[i])):\n",
    "        read_number = read_number + valid_reads[i][j][0]\n",
    "    if read_number != 0:\n",
    "        print(i)\n",
    "        print(read_number)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cc48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
